{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":383,"status":"ok","timestamp":1718267148541,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"F-SnTuHjsSib"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2863,"status":"ok","timestamp":1718267151796,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"Kx4VzIlmsVq4","outputId":"11d3ae78-a167-4491-ebc6-baca006dfc05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1718267151796,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"D4UI8bwUsSig"},"outputs":[],"source":["class TextDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","        return {\n","            'text': text,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1718267151796,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"gPOQPYIVsSih"},"outputs":[],"source":["def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n","    ds = TextDataset(\n","        texts=texts,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        num_workers=4\n","    )"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1718267151797,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"U9eAXgHdsSii"},"outputs":[],"source":["def train_epoch(model, data_loader, optimizer, device):\n","    model = model.train()\n","    losses = []\n","    correct_predictions = 0\n","    for d in data_loader:\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"label\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","        loss = outputs.loss\n","        preds = outputs.logits.argmax(dim=1)\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1718267151797,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"t6bOc7cusSij"},"outputs":[],"source":["def eval_model(model, data_loader, device):\n","    model = model.eval()\n","    losses = []\n","    correct_predictions = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"label\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","            loss = outputs.loss\n","            preds = outputs.logits.argmax(dim=1)\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses), all_preds, all_labels"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1718267151797,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"H0I-gadHsSij"},"outputs":[],"source":["def train_model(train_data_loader, val_data_loader, model, optimizer, device, epochs):\n","    history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n","\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch + 1}/{epochs}')\n","        print('-' * 10)\n","\n","        train_acc, train_loss = train_epoch(model, train_data_loader, optimizer, device)\n","        print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","        val_acc, val_loss, _, _ = eval_model(model, val_data_loader, device)\n","        print(f'Validation loss {val_loss} accuracy {val_acc}')\n","\n","        history['train_acc'].append(train_acc)\n","        history['train_loss'].append(train_loss)\n","        history['val_acc'].append(val_acc)\n","        history['val_loss'].append(val_loss)\n","\n","    return history"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1718267151798,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"CKSKhms7sSik"},"outputs":[],"source":["def preprocess(texts, tokenizer, max_len):\n","    encodings = tokenizer.batch_encode_plus(\n","        texts,\n","        add_special_tokens=True,\n","        max_length=max_len,\n","        return_token_type_ids=False,\n","        pad_to_max_length=True,\n","        return_attention_mask=True,\n","        return_tensors='pt'\n","    )\n","    return encodings"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1718267151798,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"MeiErmX1sSik"},"outputs":[],"source":["def predict(texts, model, tokenizer, max_len, device):\n","    model = model.eval()\n","    encodings = preprocess(texts, tokenizer, max_len)\n","    input_ids = encodings['input_ids'].to(device)\n","    attention_mask = encodings['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","    preds = outputs.logits.argmax(dim=1)\n","    return preds.cpu().numpy()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1718267151798,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"VHtVdTwusSil"},"outputs":[],"source":["def create_confusion_matrix(labels, preds):\n","    return confusion_matrix(labels, preds)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1718267151798,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"a21B_iKqsSim"},"outputs":[],"source":["MODEL_NAME = 'bert-base-multilingual-uncased'\n","MAX_LEN = 512\n","BATCH_SIZE = 16\n","EPOCHS = 3\n","LEARNING_RATE = 2e-5"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":632,"status":"ok","timestamp":1718267152404,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"NqVNsVvlsSim"},"outputs":[],"source":["# dir = '../Dataset Statistics/dataset'\n","dir = '/content/drive/MyDrive/Hate Speech_Multilingual /Code/Dataset Statistics/dataset'\n","\n","train_df = pd.read_csv(f'{dir}/train.csv')\n","val_df = pd.read_csv(f'{dir}/val.csv')\n","test_df = pd.read_csv(f'{dir}/test.csv')\n","\n","train_texts = train_df['text'].values\n","val_texts = val_df['text'].values\n","test_texts = test_df['text'].values\n","\n","train_labels = train_df['class'].values\n","val_labels = val_df['class'].values\n","test_labels = test_df['class'].values"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1723,"status":"ok","timestamp":1718267154116,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"r6ouzBpOsSim","outputId":"08c3e381-3456-4432-f0bc-8099aa674398"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n","model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1718267154116,"user":{"displayName":"Abdul Manaf","userId":"05210738012964930386"},"user_tz":-300},"id":"4yfcBPqZsSio","outputId":"d1e7aae2-bef9-42a1-a70b-1e017cdc2f0f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["train_data_loader = create_data_loader(train_texts, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(val_texts, val_labels, tokenizer, MAX_LEN, BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYw1VjibsSio","outputId":"d0325924-b5d6-41fa-a65c-1064d95f104f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","----------\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","history = train_model(train_data_loader, val_data_loader, model, optimizer, device, EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJi1OCCBsSio"},"outputs":[],"source":["_, _, val_preds, val_labels = eval_model(model, val_data_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGcXfzJjsSip"},"outputs":[],"source":["conf_matrix = create_confusion_matrix(val_labels, val_preds)\n","print('Confusion Matrix:')\n","print(conf_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mK-IIwu1sSip"},"outputs":[],"source":["test_texts = [\"I enjoy learning new things.\"]\n","preds = predict(test_texts, model, tokenizer, MAX_LEN, device)\n","print('Predictions:', preds)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
