{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abdul\n",
      "[nltk_data]     Manaf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import helper as hlp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "MAX_LEN = 30\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "NAME = 'xlm(roberta)-combined'\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "\n",
    "# dir = '/content/drive/MyDrive/Hate Speech_Multilingual /Code/Dataset Statistics/dataset'\n",
    "dir = '../../../Dataset Statistics/dataset'\n",
    "\n",
    "# path = f'/content/drive/MyDrive/Hate Speech_Multilingual /Code/Model training/saved_models/{NAME}'\n",
    "path = f'../saved_models/{NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{dir}/train.csv')\n",
    "val_df = pd.read_csv(f'{dir}/val.csv')\n",
    "test_df = pd.read_csv(f'{dir}/test.csv')\n",
    "\n",
    "english_train_texts = train_df['english_text'].values\n",
    "english_val_texts = val_df['english_text'].values\n",
    "english_test_texts = test_df['english_text'].values\n",
    "\n",
    "english_train_labels = train_df['class'].values\n",
    "english_val_labels = val_df['class'].values\n",
    "english_test_labels = test_df['class'].values\n",
    "\n",
    "urdu_train_texts = train_df['urdu_text'].values\n",
    "urdu_val_texts = val_df['urdu_text'].values\n",
    "urdu_test_texts = test_df['urdu_text'].values\n",
    "\n",
    "urdu_train_labels = train_df['class'].values\n",
    "urdu_val_labels = val_df['class'].values\n",
    "urdu_test_labels = test_df['class'].values\n",
    "\n",
    "sindhi_train_texts = train_df['sindhi_text'].values\n",
    "sindhi_val_texts = val_df['sindhi_text'].values\n",
    "sindhi_test_texts = test_df['sindhi_text'].values\n",
    "\n",
    "sindhi_train_labels = train_df['class'].values\n",
    "sindhi_val_labels = val_df['class'].values\n",
    "sindhi_test_labels = test_df['class'].values\n",
    "\n",
    "english_train_texts = [hlp.preprocess_text(text, language=\"english\") for text in english_train_texts]\n",
    "english_val_texts = [hlp.preprocess_text(text, language=\"english\") for text in english_val_texts]\n",
    "english_test_texts = [hlp.preprocess_text(text, language=\"english\") for text in english_test_texts]\n",
    "\n",
    "urdu_train_texts = [hlp.preprocess_text(text, language=\"urdu\") for text in urdu_train_texts]\n",
    "urdu_val_texts = [hlp.preprocess_text(text, language=\"urdu\") for text in urdu_val_texts]\n",
    "urdu_test_texts = [hlp.preprocess_text(text, language=\"urdu\") for text in urdu_test_texts]\n",
    "\n",
    "sindhi_train_texts = [hlp.preprocess_text(text, language=\"sindhi\") for text in sindhi_train_texts]\n",
    "sindhi_val_texts = [hlp.preprocess_text(text, language=\"sindhi\") for text in sindhi_val_texts]\n",
    "sindhi_test_texts = [hlp.preprocess_text(text, language=\"sindhi\") for text in sindhi_test_texts]\n",
    "\n",
    "# Concatenate text arrays\n",
    "train_texts = np.concatenate([english_train_texts, urdu_train_texts, sindhi_train_texts])\n",
    "val_texts = np.concatenate([english_val_texts, urdu_val_texts, sindhi_val_texts])\n",
    "test_texts = np.concatenate([english_test_texts, urdu_test_texts, sindhi_test_texts])\n",
    "\n",
    "# Concatenate label arrays\n",
    "train_labels = np.concatenate([english_train_labels, urdu_train_labels, sindhi_train_labels])\n",
    "val_labels = np.concatenate([english_val_labels, urdu_val_labels, sindhi_val_labels])\n",
    "test_labels = np.concatenate([english_test_labels, urdu_test_labels, sindhi_test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "train_texts, train_labels = hlp.shuffle_data(train_texts, train_labels)\n",
    "val_texts, val_labels = hlp.shuffle_data(val_texts, val_labels)\n",
    "test_texts, test_labels = hlp.shuffle_data(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hitokilling ٿي سگهي ٿو منهنجي ايندڙ رن ۾ مس مرڻ بدران ٺاهي ڊاج ڪندو پوءِ مون کي سندس نالو ياد ايندو 0\n",
      "jackposobiec date bait interviewer 0\n",
      "مَیں نے سمجھ لیا تھا جنگ دوران جنگ میں حصہ لینے میں بہت زیادہ مصروف جائے گا ۔ 0\n",
      "میں صرف ساحل تھا، کبھی بھی ویب میں بھی نہیں جا رہا تھا، میں وہاں رہنے قسم استعمال کیا جا رہا تھا 0\n",
      "reallyvirtual آپ اس فخر کرنا چاہئے سب نے کچھ اچھا کام کیا 0\n"
     ]
    }
   ],
   "source": [
    "#show 5 values along with their labels\n",
    "for i in range(5):\n",
    "    print(train_texts[i], train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## mbert model and tokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/2867 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_data_loader = hlp.create_data_loader(np.array(train_texts), np.array(train_labels), tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = hlp.create_data_loader(np.array(val_texts), np.array(val_labels), tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "history = hlp.train_model(train_data_loader, val_data_loader, model, optimizer, device, EPOCHS, tokenizer, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def extract_epoch_number(dir):\n",
    "    files = os.listdir(dir)\n",
    "    epoch = 0\n",
    "    for file in files:\n",
    "        if 'model_epoch' in file:\n",
    "            epoch = max(epoch, int(file.split('_')[-1]))\n",
    "    return epoch\n",
    "\n",
    "path = f'../saved_models/{NAME}'\n",
    "epoch = extract_epoch_number(path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{path}/tokenizer')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f'{path}/model_epoch_{epoch}', num_labels=N_CLASSES)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = hlp.create_data_loader(test_texts, np.array(test_labels), tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "test_acc, test_loss, test_label_actual, test_label_pred = hlp.eval_model(model, test_data_loader, device)\n",
    "\n",
    "print(f'Test Accuracy: {test_acc} Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.plot_confusion_matrix(test_label_actual, test_label_pred, ['Non-Hate', 'Hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\"This is the sort of thing that happens in a shithole country. @URL\"]\n",
    "preds, confidence = hlp.predict(test_texts, model, tokenizer, MAX_LEN, device)\n",
    "\n",
    "if preds[0] == 1:\n",
    "    print(f'Predicted class: Hate Speech \\nConfidence: {confidence[0][1]:.2f}')\n",
    "else:\n",
    "    print(f'Predicted class: Not Hate Speech \\nConfidence: {confidence[0][0]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
